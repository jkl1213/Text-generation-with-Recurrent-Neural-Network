{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Fortune generator.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aXvWMHUeiFup",
        "outputId": "66f89d2c-27da-4cce-fecc-ba0f7fe7ba8d"
      },
      "source": [
        "import numpy as np\n",
        "import pickle\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from google.colab import drive\n",
        "import pandas as pd\n",
        "import sklearn\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import tensorflow as tf\n",
        "import os\n",
        "import time\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 158
        },
        "id": "iMnUCp56iLJK",
        "outputId": "64c243fd-e180-4855-cf13-85c5a121a2f7"
      },
      "source": [
        "path = \"/content/drive/MyDrive/Colab Notebooks/RNN with fortune/fortunes.csv\"\n",
        "data = pd.read_csv(path)\n",
        "data = data.drop(columns=['Unnamed: 1']).dropna()\n",
        "data = data.sample(frac=1).reset_index(drop=True)\n",
        "print(len(data))\n",
        "data.head(3)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "12946\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Fortune</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>You may be tempted to interfere in someone els...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Because Taurus is one of the zodiac’s “fixed” ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>People will be demanding of you today and ther...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                             Fortune\n",
              "0  You may be tempted to interfere in someone els...\n",
              "1  Because Taurus is one of the zodiac’s “fixed” ...\n",
              "2  People will be demanding of you today and ther..."
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9xFvGVYS5HOS",
        "outputId": "c61385f6-c199-478c-dddd-6ccc38964552"
      },
      "source": [
        "raw_train_ds = tf.data.Dataset.from_tensor_slices(data[\"Fortune\"])\n",
        "next(iter(raw_train_ds))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(), dtype=string, numpy=b'You may be tempted to interfere in someone else\\xe2\\x80\\x99s personal life today but the planets warn that nothing good will come of it, so resist the urge and keep your distance. The simple fact is you cannot help everyone, nor should you want to.'>"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lRX9dhPg6Zul"
      },
      "source": [
        "text_dataset = tf.data.Dataset.from_tensor_slices(data[\"Fortune\"])\n",
        "max_features = 10000  # Maximum vocab size.\n",
        "max_len = 150  # Sequence length to pad the outputs to.\n",
        "\n",
        "# Create the layer.\n",
        "vectorize_layer = tf.keras.layers.TextVectorization(\n",
        " max_tokens=max_features,\n",
        " output_mode='int',\n",
        " output_sequence_length=max_len,\n",
        " standardize=None)\n",
        "\n",
        "# Now that the vocab layer has been created, call `adapt` on the text-only\n",
        "# dataset to create the vocabulary. You don't have to batch, but for large\n",
        "# datasets this means we're not keeping spare copies of the dataset.\n",
        "vectorize_layer.adapt(text_dataset.batch(64))"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UEWMoHum6HWX",
        "outputId": "9de2a131-1086-4165-fd36-4925a64d78d0"
      },
      "source": [
        "before = vectorize_layer.get_vocabulary()\n",
        "print(before[:5])\n",
        "after = before.copy()\n",
        "after.remove(\"[UNK]\")\n",
        "print(after[:5])"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['', '[UNK]', 'you', 'to', 'the']\n",
            "['', 'you', 'to', 'the', 'and']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "52_fovCa7Lah",
        "outputId": "b5f44cd8-f931-4b36-d62d-4729fccc7280"
      },
      "source": [
        "def vectorize_text(text):\n",
        "  text = tf.expand_dims(text, -1)\n",
        "  return vectorize_layer(text)\n",
        "print(vectorize_text(next(iter(raw_train_ds))))\n",
        "print(next(iter(raw_train_ds)))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[[  18   23    8  388    3 2827   14   94 1644  405   82   42   16    4\n",
            "    93  202   12  175   56   10  105    7  416   25  913    4  553    5\n",
            "   131    6 2069   37  465  183   11    2  179  170 2235 1468   76    2\n",
            "    80  569    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0]], shape=(1, 150), dtype=int64)\n",
            "tf.Tensor(b'You may be tempted to interfere in someone else\\xe2\\x80\\x99s personal life today but the planets warn that nothing good will come of it, so resist the urge and keep your distance. The simple fact is you cannot help everyone, nor should you want to.', shape=(), dtype=string)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MimHk2Ix7Rqg",
        "outputId": "00fefbf5-79ae-4073-f994-4a97bebad819"
      },
      "source": [
        "ids_from_chars = tf.keras.layers.StringLookup(vocabulary=after)\n",
        "chars_from_ids = tf.keras.layers.StringLookup(vocabulary=after, invert=True)\n",
        "ids_from_chars.get_vocabulary()[:5]"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['[UNK]', '', 'you', 'to', 'the']"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GTQIgFvy7WI3",
        "outputId": "995b3413-e003-44e0-ce0e-ee1bb0b5f1e5"
      },
      "source": [
        "chars_sample = chars_from_ids(vectorize_text(next(iter(raw_train_ds))))\n",
        "print(chars_sample)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[[b'You' b'may' b'be' b'tempted' b'to' b'interfere' b'in' b'someone'\n",
            "  b'else\\xe2\\x80\\x99s' b'personal' b'life' b'today' b'but' b'the'\n",
            "  b'planets' b'warn' b'that' b'nothing' b'good' b'will' b'come' b'of'\n",
            "  b'it,' b'so' b'resist' b'the' b'urge' b'and' b'keep' b'your'\n",
            "  b'distance.' b'The' b'simple' b'fact' b'is' b'you' b'cannot' b'help'\n",
            "  b'everyone,' b'nor' b'should' b'you' b'want' b'to.' b'[UNK]' b'[UNK]'\n",
            "  b'[UNK]' b'[UNK]' b'[UNK]' b'[UNK]' b'[UNK]' b'[UNK]' b'[UNK]' b'[UNK]'\n",
            "  b'[UNK]' b'[UNK]' b'[UNK]' b'[UNK]' b'[UNK]' b'[UNK]' b'[UNK]' b'[UNK]'\n",
            "  b'[UNK]' b'[UNK]' b'[UNK]' b'[UNK]' b'[UNK]' b'[UNK]' b'[UNK]' b'[UNK]'\n",
            "  b'[UNK]' b'[UNK]' b'[UNK]' b'[UNK]' b'[UNK]' b'[UNK]' b'[UNK]' b'[UNK]'\n",
            "  b'[UNK]' b'[UNK]' b'[UNK]' b'[UNK]' b'[UNK]' b'[UNK]' b'[UNK]' b'[UNK]'\n",
            "  b'[UNK]' b'[UNK]' b'[UNK]' b'[UNK]' b'[UNK]' b'[UNK]' b'[UNK]' b'[UNK]'\n",
            "  b'[UNK]' b'[UNK]' b'[UNK]' b'[UNK]' b'[UNK]' b'[UNK]' b'[UNK]' b'[UNK]'\n",
            "  b'[UNK]' b'[UNK]' b'[UNK]' b'[UNK]' b'[UNK]' b'[UNK]' b'[UNK]' b'[UNK]'\n",
            "  b'[UNK]' b'[UNK]' b'[UNK]' b'[UNK]' b'[UNK]' b'[UNK]' b'[UNK]' b'[UNK]'\n",
            "  b'[UNK]' b'[UNK]' b'[UNK]' b'[UNK]' b'[UNK]' b'[UNK]' b'[UNK]' b'[UNK]'\n",
            "  b'[UNK]' b'[UNK]' b'[UNK]' b'[UNK]' b'[UNK]' b'[UNK]' b'[UNK]' b'[UNK]'\n",
            "  b'[UNK]' b'[UNK]' b'[UNK]' b'[UNK]' b'[UNK]' b'[UNK]' b'[UNK]' b'[UNK]'\n",
            "  b'[UNK]' b'[UNK]' b'[UNK]' b'[UNK]' b'[UNK]' b'[UNK]' b'[UNK]' b'[UNK]']], shape=(1, 150), dtype=string)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BqhVfBdz7pMM",
        "outputId": "75660141-c4cc-42bd-f71a-30023a7ea258"
      },
      "source": [
        "ids_from_chars(chars_sample)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1, 150), dtype=int64, numpy=\n",
              "array([[  18,   23,    8,  388,    3, 2827,   14,   94, 1644,  405,   82,\n",
              "          42,   16,    4,   93,  202,   12,  175,   56,   10,  105,    7,\n",
              "         416,   25,  913,    4,  553,    5,  131,    6, 2069,   37,  465,\n",
              "         183,   11,    2,  179,  170, 2235, 1468,   76,    2,   80,  569,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0]])>"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "wExx2p5770Do",
        "outputId": "8a556d17-1041-4cea-ebdf-756a497fcdfe"
      },
      "source": [
        "ids_from_chars.get_vocabulary()[94]"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'someone'"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WrR6aB7m74g3",
        "outputId": "e8782c73-b483-4c00-c8b4-3e262c4accad"
      },
      "source": [
        "chars_sample = chars_from_ids(vectorize_text(next(iter(raw_train_ds))))\n",
        "print(chars_sample)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[[b'You' b'may' b'be' b'tempted' b'to' b'interfere' b'in' b'someone'\n",
            "  b'else\\xe2\\x80\\x99s' b'personal' b'life' b'today' b'but' b'the'\n",
            "  b'planets' b'warn' b'that' b'nothing' b'good' b'will' b'come' b'of'\n",
            "  b'it,' b'so' b'resist' b'the' b'urge' b'and' b'keep' b'your'\n",
            "  b'distance.' b'The' b'simple' b'fact' b'is' b'you' b'cannot' b'help'\n",
            "  b'everyone,' b'nor' b'should' b'you' b'want' b'to.' b'[UNK]' b'[UNK]'\n",
            "  b'[UNK]' b'[UNK]' b'[UNK]' b'[UNK]' b'[UNK]' b'[UNK]' b'[UNK]' b'[UNK]'\n",
            "  b'[UNK]' b'[UNK]' b'[UNK]' b'[UNK]' b'[UNK]' b'[UNK]' b'[UNK]' b'[UNK]'\n",
            "  b'[UNK]' b'[UNK]' b'[UNK]' b'[UNK]' b'[UNK]' b'[UNK]' b'[UNK]' b'[UNK]'\n",
            "  b'[UNK]' b'[UNK]' b'[UNK]' b'[UNK]' b'[UNK]' b'[UNK]' b'[UNK]' b'[UNK]'\n",
            "  b'[UNK]' b'[UNK]' b'[UNK]' b'[UNK]' b'[UNK]' b'[UNK]' b'[UNK]' b'[UNK]'\n",
            "  b'[UNK]' b'[UNK]' b'[UNK]' b'[UNK]' b'[UNK]' b'[UNK]' b'[UNK]' b'[UNK]'\n",
            "  b'[UNK]' b'[UNK]' b'[UNK]' b'[UNK]' b'[UNK]' b'[UNK]' b'[UNK]' b'[UNK]'\n",
            "  b'[UNK]' b'[UNK]' b'[UNK]' b'[UNK]' b'[UNK]' b'[UNK]' b'[UNK]' b'[UNK]'\n",
            "  b'[UNK]' b'[UNK]' b'[UNK]' b'[UNK]' b'[UNK]' b'[UNK]' b'[UNK]' b'[UNK]'\n",
            "  b'[UNK]' b'[UNK]' b'[UNK]' b'[UNK]' b'[UNK]' b'[UNK]' b'[UNK]' b'[UNK]'\n",
            "  b'[UNK]' b'[UNK]' b'[UNK]' b'[UNK]' b'[UNK]' b'[UNK]' b'[UNK]' b'[UNK]'\n",
            "  b'[UNK]' b'[UNK]' b'[UNK]' b'[UNK]' b'[UNK]' b'[UNK]' b'[UNK]' b'[UNK]'\n",
            "  b'[UNK]' b'[UNK]' b'[UNK]' b'[UNK]' b'[UNK]' b'[UNK]' b'[UNK]' b'[UNK]']], shape=(1, 150), dtype=string)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0qoe_M2J8FHH"
      },
      "source": [
        "def get_dataset_partitions_tf(ds, ds_size, train_split=0.8, val_split=0.1, test_split=0.1, shuffle=True, shuffle_size=10000):\n",
        "    assert (train_split + test_split + val_split) == 1\n",
        "    \n",
        "    if shuffle:\n",
        "        # Specify seed to always have the same split distribution between runs\n",
        "        ds = ds.shuffle(shuffle_size, seed=12)\n",
        "    \n",
        "    train_size = int(train_split * ds_size)\n",
        "    val_size = int(val_split * ds_size)\n",
        "    \n",
        "    train_ds = ds.take(train_size)    \n",
        "    val_ds = ds.skip(train_size).take(val_size)\n",
        "    test_ds = ds.skip(train_size).skip(val_size)\n",
        "    \n",
        "    return train_ds, val_ds, test_ds\n",
        "ds = raw_train_ds.map(vectorize_text)\n"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PKhNYKer8OXg",
        "outputId": "0ff1f078-8e8c-41e6-e23f-43705ff53f8c"
      },
      "source": [
        "#ds is now [[1,4,12,8]]\n",
        "for i in ds.take(1):\n",
        "  print(i)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[[  18   23    8  388    3 2827   14   94 1644  405   82   42   16    4\n",
            "    93  202   12  175   56   10  105    7  416   25  913    4  553    5\n",
            "   131    6 2069   37  465  183   11    2  179  170 2235 1468   76    2\n",
            "    80  569    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0]], shape=(1, 150), dtype=int64)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V1gcm_ng8QXr",
        "outputId": "12c4200d-21d6-4254-dd26-133c084fc00d"
      },
      "source": [
        "for i in ds.take(1):\n",
        "  print(chars_from_ids(i))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[[b'You' b'may' b'be' b'tempted' b'to' b'interfere' b'in' b'someone'\n",
            "  b'else\\xe2\\x80\\x99s' b'personal' b'life' b'today' b'but' b'the'\n",
            "  b'planets' b'warn' b'that' b'nothing' b'good' b'will' b'come' b'of'\n",
            "  b'it,' b'so' b'resist' b'the' b'urge' b'and' b'keep' b'your'\n",
            "  b'distance.' b'The' b'simple' b'fact' b'is' b'you' b'cannot' b'help'\n",
            "  b'everyone,' b'nor' b'should' b'you' b'want' b'to.' b'[UNK]' b'[UNK]'\n",
            "  b'[UNK]' b'[UNK]' b'[UNK]' b'[UNK]' b'[UNK]' b'[UNK]' b'[UNK]' b'[UNK]'\n",
            "  b'[UNK]' b'[UNK]' b'[UNK]' b'[UNK]' b'[UNK]' b'[UNK]' b'[UNK]' b'[UNK]'\n",
            "  b'[UNK]' b'[UNK]' b'[UNK]' b'[UNK]' b'[UNK]' b'[UNK]' b'[UNK]' b'[UNK]'\n",
            "  b'[UNK]' b'[UNK]' b'[UNK]' b'[UNK]' b'[UNK]' b'[UNK]' b'[UNK]' b'[UNK]'\n",
            "  b'[UNK]' b'[UNK]' b'[UNK]' b'[UNK]' b'[UNK]' b'[UNK]' b'[UNK]' b'[UNK]'\n",
            "  b'[UNK]' b'[UNK]' b'[UNK]' b'[UNK]' b'[UNK]' b'[UNK]' b'[UNK]' b'[UNK]'\n",
            "  b'[UNK]' b'[UNK]' b'[UNK]' b'[UNK]' b'[UNK]' b'[UNK]' b'[UNK]' b'[UNK]'\n",
            "  b'[UNK]' b'[UNK]' b'[UNK]' b'[UNK]' b'[UNK]' b'[UNK]' b'[UNK]' b'[UNK]'\n",
            "  b'[UNK]' b'[UNK]' b'[UNK]' b'[UNK]' b'[UNK]' b'[UNK]' b'[UNK]' b'[UNK]'\n",
            "  b'[UNK]' b'[UNK]' b'[UNK]' b'[UNK]' b'[UNK]' b'[UNK]' b'[UNK]' b'[UNK]'\n",
            "  b'[UNK]' b'[UNK]' b'[UNK]' b'[UNK]' b'[UNK]' b'[UNK]' b'[UNK]' b'[UNK]'\n",
            "  b'[UNK]' b'[UNK]' b'[UNK]' b'[UNK]' b'[UNK]' b'[UNK]' b'[UNK]' b'[UNK]'\n",
            "  b'[UNK]' b'[UNK]' b'[UNK]' b'[UNK]' b'[UNK]' b'[UNK]' b'[UNK]' b'[UNK]']], shape=(1, 150), dtype=string)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3PnfWdFs8S4i"
      },
      "source": [
        "def split_input_target(sequence):\n",
        "    input_text = sequence[0][:-1]\n",
        "    target_text = sequence[0][1:]\n",
        "    return input_text, target_text\n",
        "\n",
        "#create input, output for each sample\n",
        "dataset = ds.map(split_input_target)\n",
        "train_ds, val_ds, test_ds = get_dataset_partitions_tf(dataset, len(data), shuffle=False)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AggEkIRf8cCM",
        "outputId": "076fc847-03b2-4eea-af12-066446338dc9"
      },
      "source": [
        "for i in test_ds.take(1):\n",
        "  print(i)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(<tf.Tensor: shape=(149,), dtype=int64, numpy=\n",
            "array([  18,   10,   27,    4,  241,    3,  389,   21,    2,   38,   28,\n",
            "         42,    5,    2,   66, 1719,   13,    5,  364,   68,   44,    2,\n",
            "        487,  275,    9, 2563,  558,    3, 1129,    2,   98,    2,   23,\n",
            "        423,    4, 3945,    7,    4,  268, 4494,   50, 1084,   19,  191,\n",
            "         25, 6572,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "          0,    0,    0,    0,    0,    0])>, <tf.Tensor: shape=(149,), dtype=int64, numpy=\n",
            "array([  10,   27,    4,  241,    3,  389,   21,    2,   38,   28,   42,\n",
            "          5,    2,   66, 1719,   13,    5,  364,   68,   44,    2,  487,\n",
            "        275,    9, 2563,  558,    3, 1129,    2,   98,    2,   23,  423,\n",
            "          4, 3945,    7,    4,  268, 4494,   50, 1084,   19,  191,   25,\n",
            "       6572,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "          0,    0,    0,    0,    0,    0])>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kIMhVPeP8gTF",
        "outputId": "b32d9280-95a4-470e-bcb4-b65b2a7b2453"
      },
      "source": [
        "# Batch size\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "# Buffer size to shuffle the dataset\n",
        "# (TF data is designed to work with possibly infinite sequences,\n",
        "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
        "# it maintains a buffer in which it shuffles elements).\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "dataset = (\n",
        "    dataset\n",
        "    .shuffle(BUFFER_SIZE)\n",
        "    .batch(BATCH_SIZE, drop_remainder=True)\n",
        "    .prefetch(tf.data.experimental.AUTOTUNE))\n",
        "\n",
        "dataset"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<PrefetchDataset shapes: ((64, 149), (64, 149)), types: (tf.int64, tf.int64)>"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ew8LaY6K8k9S"
      },
      "source": [
        "# Length of the vocabulary in chars\n",
        "vocab_size = len(ids_from_chars.get_vocabulary())\n",
        "\n",
        "# The embedding dimension\n",
        "embedding_dim = 256\n",
        "\n",
        "# Number of RNN units\n",
        "rnn_units = 1024"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9fuWaRe18tib"
      },
      "source": [
        "class MyModel(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
        "    super().__init__(self)\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru = tf.keras.layers.GRU(rnn_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True)\n",
        "    self.dense = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "  def call(self, inputs, states=None, return_state=False, training=False):\n",
        "    x = inputs\n",
        "    x = self.embedding(x, training=training)\n",
        "    if states is None:\n",
        "      states = self.gru.get_initial_state(x)\n",
        "    x, states = self.gru(x, initial_state=states, training=training)\n",
        "    x = self.dense(x, training=training)\n",
        "\n",
        "    if return_state:\n",
        "      return x, states\n",
        "    else:\n",
        "      return x"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z_qYQng48wFq"
      },
      "source": [
        "model = MyModel(\n",
        "    # Be sure the vocabulary size matches the `StringLookup` layers.\n",
        "    vocab_size=len(ids_from_chars.get_vocabulary()),\n",
        "    embedding_dim=embedding_dim,\n",
        "    rnn_units=rnn_units)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GmsIcaNm8ysp",
        "outputId": "ba87cd78-479c-45cc-c465-8120fa4f0734"
      },
      "source": [
        "for input_example_batch, target_example_batch in dataset.take(1):\n",
        "    example_batch_predictions = model(input_example_batch)\n",
        "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(64, 149, 10000) # (batch_size, sequence_length, vocab_size)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2wLrzfVU81Pv",
        "outputId": "85c22528-bacd-4f32-e483-2106a60fe6a1"
      },
      "source": [
        "model.summary()\n"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"my_model\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        multiple                  2560000   \n",
            "_________________________________________________________________\n",
            "gru (GRU)                    multiple                  3938304   \n",
            "_________________________________________________________________\n",
            "dense (Dense)                multiple                  10250000  \n",
            "=================================================================\n",
            "Total params: 16,748,304\n",
            "Trainable params: 16,748,304\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cbDG2gKV9Gfl"
      },
      "source": [
        "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
        "sampled_indices = tf.squeeze(sampled_indices, axis=-1).numpy()"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jB5rl9lr9PoT",
        "outputId": "97045a8e-59cb-4b6c-cc9c-e9ef537e346c"
      },
      "source": [
        "sampled_indices"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([3115, 7158, 4688, 6540, 4049, 7120, 5175, 6229, 9713, 6995, 5414,\n",
              "       4747, 2046, 7084, 2122, 9182, 6685, 8580, 5914,  812, 1901, 9380,\n",
              "       9082, 5003, 6371,  764,  968, 7373, 8304, 1413, 6818, 9217, 5665,\n",
              "       5759, 1417, 3179, 9981, 1069, 1631, 2182, 7517, 4150, 5502, 4096,\n",
              "       8539, 3021,  361,  505, 3218, 8607,  293, 5718, 1366, 4367, 1030,\n",
              "       8320, 6841, 3998, 6409,  717, 9705, 2338, 7270, 4803, 5067, 1969,\n",
              "       7597, 5885,   52,  704,   75, 9099, 8579, 3189, 1301, 4905, 2005,\n",
              "       5411, 1539, 4984, 8574, 9142, 9818, 9728, 2963, 4472, 9701, 2780,\n",
              "        380, 4480, 9462, 4264, 4461, 3015, 3695, 1247, 8738, 2644, 3309,\n",
              "       6047, 1903, 2166, 7881, 6295, 8010, 4606, 2287, 7670, 5198, 8460,\n",
              "        309, 5673, 2396, 2506,  185, 4502, 4670, 1563, 7320, 2847, 7263,\n",
              "         88, 9165, 7381, 3157, 5520, 6902, 2606, 9973, 1780, 3243, 4326,\n",
              "        974, 9424,  197, 9466, 1832,   10, 4013, 6995, 8256,  264, 3041,\n",
              "       8173, 2074, 4015,  263, 3563, 5295])"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RhyaZnvh9Raz",
        "outputId": "cfeb0a69-afa6-4683-c49d-e4902f3d0067"
      },
      "source": [
        "print(\"Input:\\n\", chars_from_ids(input_example_batch[0]))\n",
        "print()\n",
        "print(\"Next Char Predictions:\\n\", chars_from_ids(sampled_indices))"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input:\n",
            " tf.Tensor(\n",
            "[b'Something' b'will' b'happen' b'that' b'catches' b'you' b'by'\n",
            " b'surprise' b'over' b'the' b'next' b'24' b'hours.' b'It' b'may' b'not'\n",
            " b'be' b'the' b'kind' b'of' b'surprise' b'you' b'enjoy' b'very' b'much'\n",
            " b'but' b'if' b'you' b'are' b'honest' b'you' b'will' b'admit,' b'if'\n",
            " b'only' b'to' b'yourself,' b'that' b'you' b'needed' b'a' b'shock' b'to'\n",
            " b'your' b'system.' b'[UNK]' b'[UNK]' b'[UNK]' b'[UNK]' b'[UNK]' b'[UNK]'\n",
            " b'[UNK]' b'[UNK]' b'[UNK]' b'[UNK]' b'[UNK]' b'[UNK]' b'[UNK]' b'[UNK]'\n",
            " b'[UNK]' b'[UNK]' b'[UNK]' b'[UNK]' b'[UNK]' b'[UNK]' b'[UNK]' b'[UNK]'\n",
            " b'[UNK]' b'[UNK]' b'[UNK]' b'[UNK]' b'[UNK]' b'[UNK]' b'[UNK]' b'[UNK]'\n",
            " b'[UNK]' b'[UNK]' b'[UNK]' b'[UNK]' b'[UNK]' b'[UNK]' b'[UNK]' b'[UNK]'\n",
            " b'[UNK]' b'[UNK]' b'[UNK]' b'[UNK]' b'[UNK]' b'[UNK]' b'[UNK]' b'[UNK]'\n",
            " b'[UNK]' b'[UNK]' b'[UNK]' b'[UNK]' b'[UNK]' b'[UNK]' b'[UNK]' b'[UNK]'\n",
            " b'[UNK]' b'[UNK]' b'[UNK]' b'[UNK]' b'[UNK]' b'[UNK]' b'[UNK]' b'[UNK]'\n",
            " b'[UNK]' b'[UNK]' b'[UNK]' b'[UNK]' b'[UNK]' b'[UNK]' b'[UNK]' b'[UNK]'\n",
            " b'[UNK]' b'[UNK]' b'[UNK]' b'[UNK]' b'[UNK]' b'[UNK]' b'[UNK]' b'[UNK]'\n",
            " b'[UNK]' b'[UNK]' b'[UNK]' b'[UNK]' b'[UNK]' b'[UNK]' b'[UNK]' b'[UNK]'\n",
            " b'[UNK]' b'[UNK]' b'[UNK]' b'[UNK]' b'[UNK]' b'[UNK]' b'[UNK]' b'[UNK]'\n",
            " b'[UNK]' b'[UNK]' b'[UNK]' b'[UNK]' b'[UNK]' b'[UNK]' b'[UNK]' b'[UNK]'\n",
            " b'[UNK]' b'[UNK]'], shape=(149,), dtype=string)\n",
            "\n",
            "Next Char Predictions:\n",
            " tf.Tensor(\n",
            "[b'secure' b'pound' b'attraction' b'wins' b'hundred' b'promised.'\n",
            " b'eating' b'contest' b'self-confidence.' b'ripple'\n",
            " b'\\xe2\\x80\\x9cbad\\xe2\\x80\\x9d.' b'Group' b'overreact' b'raw' b'pleasant'\n",
            " b'tenfold,' b'tons' b'\\xe2\\x80\\x9cI\\xe2\\x80\\x99ll' b'know-how'\n",
            " b'interesting' b'matters,' b'stiff' b'touchy!' b'noble' b'arises.'\n",
            " b'huge' b'Mars,' b'memory' b'aggression' b'we' b'standstill' b'tapestry'\n",
            " b'route.' b'practical.' b'state' b'inch' b'refrain' b'unexpected'\n",
            " b'ideas.' b'who,' b'intended' b'buck' b'treading' b'employer'\n",
            " b'\\xe2\\x80\\x9ctake' b'corner' b'was' b'back.' b'essential.' b'yearning'\n",
            " b'working' b'radically' b'After' b'sad.' b'quiet' b'adopt.' b'sorrow.'\n",
            " b'occasions.' b'achieving' b'Everyone' b'self-doubt,' b'effort,' b'one?'\n",
            " b'turning.' b'interested.' b'provoke' b'hopefully,' b'low.' b'them'\n",
            " b'possible.' b'Don\\xe2\\x80\\x99t' b'tolerance'\n",
            " b'\\xe2\\x80\\x9cNice\\xe2\\x80\\x9d' b'higher.' b'ears' b'right!' b'bite'\n",
            " b'\\xe2\\x80\\x9cme\\xe2\\x80\\x9d' b'reveal' b'original,'\n",
            " b'\\xe2\\x80\\x9cavoid\\xe2\\x80\\x9d' b'threatening-looking' b'route?'\n",
            " b'seductive,' b'moves.' b'loud.' b'self-evident' b'switch'\n",
            " b'you\\xe2\\x80\\x99re' b'lingering' b'spending.' b'wear' b'moons'\n",
            " b'definitely' b'gift' b'recently' b'way-out' b'praise' b'venture'\n",
            " b'frees' b'life\\xe2\\x80\\x99s' b'before,' b'drives' b'caring' b'cover.'\n",
            " b'difference?' b'skills' b'gracious' b'destiny.' b'Frankly,' b'easily'\n",
            " b'rigidly' b'party.' b'responsibilities.' b'move' b'invited' b'bringing'\n",
            " b'Work' b'nervous' b'fashion' b'ordered' b'been' b'them;' b'measuring'\n",
            " b'modest' b'thrill' b'shoot' b'wander' b'register.' b'shut'\n",
            " b'considerably' b'stayed' b'rules' b'squaring' b'later' b'spectrum'\n",
            " b'painful' b'will' b'mind\\xe2\\x80\\x99s' b'ripple' b'arrogance' b'while'\n",
            " b'attach' b'bother?' b'dark' b'mentally.' b'focus' b'tool' b'beginning.'], shape=(149,), dtype=string)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4N63eKTG9Uiy"
      },
      "source": [
        "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eY3hNjQf9cjQ",
        "outputId": "b3903c3c-57f6-4259-ac20-da58e0e150d2"
      },
      "source": [
        "example_batch_loss = loss(target_example_batch, example_batch_predictions)\n",
        "mean_loss = example_batch_loss.numpy().mean()\n",
        "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
        "print(\"Mean loss:        \", mean_loss)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction shape:  (64, 149, 10000)  # (batch_size, sequence_length, vocab_size)\n",
            "Mean loss:         9.207552\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dYmVvdn79eLw",
        "outputId": "7e0cf324-20c5-4049-ec40-d07c6e9d8c41"
      },
      "source": [
        "tf.exp(mean_loss).numpy()"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "9972.154"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3q_7dTdM9fvj"
      },
      "source": [
        "\n",
        "model.compile(optimizer='adam', loss=loss)"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "um7n0nzO9iAu"
      },
      "source": [
        "import os\n",
        "# Directory where the checkpoints will be saved\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "# Name of the checkpoint files\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True)"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xCKquL029je8",
        "outputId": "8c510983-e12b-4263-ba9b-cbef6edf9209"
      },
      "source": [
        "EPOCHS = 5\n",
        "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "202/202 [==============================] - 105s 514ms/step - loss: 0.2741\n",
            "Epoch 2/5\n",
            "202/202 [==============================] - 105s 510ms/step - loss: 0.2497\n",
            "Epoch 3/5\n",
            "202/202 [==============================] - 105s 510ms/step - loss: 0.2280\n",
            "Epoch 4/5\n",
            "202/202 [==============================] - 106s 515ms/step - loss: 0.2085\n",
            "Epoch 5/5\n",
            "202/202 [==============================] - 105s 513ms/step - loss: 0.1908\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vr0SpXat9lvJ"
      },
      "source": [
        "class OneStep(tf.keras.Model):\n",
        "  def __init__(self, model, chars_from_ids, ids_from_chars, temperature=1.0):\n",
        "    super().__init__()\n",
        "    self.temperature = temperature\n",
        "    self.model = model\n",
        "    self.chars_from_ids = chars_from_ids\n",
        "    self.ids_from_chars = ids_from_chars\n",
        "\n",
        "    # Create a mask to prevent \"[UNK]\" from being generated.\n",
        "    skip_ids = self.ids_from_chars(['[UNK]'])[:, None]\n",
        "    sparse_mask = tf.SparseTensor(\n",
        "        # Put a -inf at each bad index.\n",
        "        values=[-float('inf')]*len(skip_ids),\n",
        "        indices=skip_ids,\n",
        "        # Match the shape to the vocabulary\n",
        "        dense_shape=[len(ids_from_chars.get_vocabulary())])\n",
        "    self.prediction_mask = tf.sparse.to_dense(sparse_mask)\n",
        "\n",
        "  @tf.function\n",
        "  def generate_one_step(self, inputs, states=None):\n",
        "    # Convert strings to token IDs. [\"HK HSBC\"] -> [\"HK\",\"HSBC\"]\n",
        "    x = tf.strings.split(\n",
        "    inputs, sep=\" \", maxsplit=-1, name=None)\n",
        "    #[\"HK\",\"HSBC\"] -> [14,15]\n",
        "    input_ids = ids_from_chars(x).to_tensor()\n",
        "    #print(input_ids)\n",
        "\n",
        "    # Run the model.\n",
        "    # predicted_logits.shape is [batch, char, next_char_logits]\n",
        "    predicted_logits, states = self.model(inputs=input_ids, states=states,\n",
        "                                          return_state=True)\n",
        "    # Only use the last prediction.\n",
        "    predicted_logits = predicted_logits[:, -1, :]\n",
        "    predicted_logits = predicted_logits/self.temperature\n",
        "    # Apply the prediction mask: prevent \"[UNK]\" from being generated.\n",
        "    predicted_logits = predicted_logits + self.prediction_mask\n",
        "\n",
        "    # Sample the output logits to generate token IDs.\n",
        "    predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
        "    predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n",
        "\n",
        "    # Convert from token ids to characters\n",
        "    predicted_chars = self.chars_from_ids(predicted_ids)\n",
        "\n",
        "    # Return the characters and model state.\n",
        "    return predicted_chars, states"
      ],
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c0MW5-tPIKo5"
      },
      "source": [
        "one_step_model = OneStep(model, chars_from_ids, ids_from_chars)"
      ],
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "Mijim52QIMWC",
        "outputId": "0c4fac41-fc27-4495-a7ae-253c317d2c5d"
      },
      "source": [
        "import time\n",
        "start = time.time()\n",
        "states = None\n",
        "next_char = [\"Try\"]\n",
        "result = [next_char]\n",
        "#print(len(next_char[0].split()))\n",
        "end = 0\n",
        "for n in range(50):\n",
        "  if end == 0:\n",
        "    next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
        "    result.append(next_char)\n",
        "    try: \n",
        "      if next_char.numpy()[0].decode('utf-8')[-1] == \".\":\n",
        "        end = 1\n",
        "    except:\n",
        "      print(next_char.numpy()[0].decode('utf-8'))\n",
        "      end = 1\n",
        "  #print(next_char.numpy()[0].decode('utf-8'))\n",
        "result = tf.strings.join(result,separator=' ')\n",
        "end = time.time()\n",
        "#print(result[0])\n",
        "#print('\\nRun time:', end - start)\n",
        "result.numpy()[0].decode('utf-8')\n"
      ],
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Try not to get impatient with people and do whatever it takes to get along with them today.'"
            ]
          },
          "metadata": {},
          "execution_count": 121
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iW8IIUaWOqsh"
      },
      "source": [
        "#os.listdir(checkpoint_dir)"
      ],
      "execution_count": 117,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "0lQnWMx-O33q",
        "outputId": "2efc366e-d1d6-445f-c5bf-bd0a7bb6bcd9"
      },
      "source": [
        "latest = tf.train.latest_checkpoint(checkpoint_dir)\n",
        "latest"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'./training_checkpoints/ckpt_5'"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6lyZJG-TPAUt"
      },
      "source": [
        "# Save the weights\n",
        "model.save_weights('/content/drive/MyDrive/Colab Notebooks/RNN with fortune/my_check_point')\n",
        "\n"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bgQ3o7EbRJI2"
      },
      "source": [
        "### Reusing model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f50EZRiVQDxL"
      },
      "source": [
        "class MyModel(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
        "    super().__init__(self)\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru = tf.keras.layers.GRU(rnn_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True)\n",
        "    self.dense = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "  def call(self, inputs, states=None, return_state=False, training=False):\n",
        "    x = inputs\n",
        "    x = self.embedding(x, training=training)\n",
        "    if states is None:\n",
        "      states = self.gru.get_initial_state(x)\n",
        "    x, states = self.gru(x, initial_state=states, training=training)\n",
        "    x = self.dense(x, training=training)\n",
        "\n",
        "    if return_state:\n",
        "      return x, states\n",
        "    else:\n",
        "      return x\n",
        "#create new model\n",
        "new_mod = MyModel(\n",
        "    # Be sure the vocabulary size matches the `StringLookup` layers.\n",
        "    vocab_size=len(ids_from_chars.get_vocabulary()),\n",
        "    embedding_dim=embedding_dim,\n",
        "    rnn_units=rnn_units)\n",
        "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "new_mod.compile(optimizer='adam', loss=loss)\n",
        "# Restore the weights\n",
        "new_mod.load_weights('/content/drive/MyDrive/Colab Notebooks/RNN with fortune/my_check_point')\n",
        "class OneStep(tf.keras.Model):\n",
        "  def __init__(self, model, chars_from_ids, ids_from_chars, temperature=1.0):\n",
        "    super().__init__()\n",
        "    self.temperature = temperature\n",
        "    self.model = model\n",
        "    self.chars_from_ids = chars_from_ids\n",
        "    self.ids_from_chars = ids_from_chars\n",
        "\n",
        "    # Create a mask to prevent \"[UNK]\" from being generated.\n",
        "    skip_ids = self.ids_from_chars(['[UNK]'])[:, None]\n",
        "    sparse_mask = tf.SparseTensor(\n",
        "        # Put a -inf at each bad index.\n",
        "        values=[-float('inf')]*len(skip_ids),\n",
        "        indices=skip_ids,\n",
        "        # Match the shape to the vocabulary\n",
        "        dense_shape=[len(ids_from_chars.get_vocabulary())])\n",
        "    self.prediction_mask = tf.sparse.to_dense(sparse_mask)\n",
        "\n",
        "  @tf.function\n",
        "  def generate_one_step(self, inputs, states=None):\n",
        "    # Convert strings to token IDs. [\"HK HSBC\"] -> [\"HK\",\"HSBC\"]\n",
        "    x = tf.strings.split(\n",
        "    inputs, sep=\" \", maxsplit=-1, name=None)\n",
        "    #[\"HK\",\"HSBC\"] -> [14,15]\n",
        "    input_ids = ids_from_chars(x).to_tensor()\n",
        "    print(input_ids)\n",
        "\n",
        "    # Run the model.\n",
        "    # predicted_logits.shape is [batch, char, next_char_logits]\n",
        "    predicted_logits, states = self.model(inputs=input_ids, states=states,\n",
        "                                          return_state=True)\n",
        "    # Only use the last prediction.\n",
        "    predicted_logits = predicted_logits[:, -1, :]\n",
        "    predicted_logits = predicted_logits/self.temperature\n",
        "    # Apply the prediction mask: prevent \"[UNK]\" from being generated.\n",
        "    predicted_logits = predicted_logits + self.prediction_mask\n",
        "\n",
        "    # Sample the output logits to generate token IDs.\n",
        "    predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
        "    predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n",
        "\n",
        "    # Convert from token ids to characters\n",
        "    predicted_chars = self.chars_from_ids(predicted_ids)\n",
        "\n",
        "    # Return the characters and model state.\n",
        "    return predicted_chars, states\n",
        "one_step_model = OneStep(new_mod, chars_from_ids, ids_from_chars)\n"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZRCgs9kuRDeG",
        "outputId": "8e04ec30-cece-4295-f096-a964ac27ed26"
      },
      "source": [
        "import time\n",
        "start = time.time()\n",
        "states = None\n",
        "next_char = [\"Why is it\"]\n",
        "result = [next_char]\n",
        "print(len(next_char[0].split()))\n",
        "for n in range(50):\n",
        "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
        "  result.append(next_char)\n",
        "  #print(next_char)\n",
        "result = tf.strings.join(result,separator=' ')\n",
        "end = time.time()\n",
        "print(result[0])\n",
        "print('\\nRun time:', end - start)"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3\n",
            "Tensor(\"RaggedToTensor/RaggedTensorToTensor:0\", shape=(1, None), dtype=int64)\n",
            "Tensor(\"RaggedToTensor/RaggedTensorToTensor:0\", shape=(1, None), dtype=int64)\n",
            "Tensor(\"RaggedToTensor/RaggedTensorToTensor:0\", shape=(None, None), dtype=int64)\n",
            "tf.Tensor(b'Why is it that you intend to do so much about what might  \\xe2\\x80\\x93 so long as the year is  things in yourself? No matter where it may be that certain people are being annoyed over the course and it\\xe2\\x80\\x99s the only way you will have to go along the flow.', shape=(), dtype=string)\n",
            "\n",
            "Run time: 1.7247834205627441\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aySgOTymRFYA"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}